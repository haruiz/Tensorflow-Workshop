{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-automatic-Differentiate.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORuCTG/iBFeoJzUs9yyzap"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U3EWsLxD4TC3","colab_type":"text"},"source":["*To remember!:  Derivative, in mathematics,the rate of change of a function with respect to a variable.*\n","\n","# Automatic Differentiate\n","\n","In mathematics and computer algebra: automatic differentiation (AD), also called algorithmic differentiation, computational differentiation[1][2], auto-differentiation, or simply autodiff, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. Multivariable Calculus\n","\n","The neural networks relies on back propagations to calculate gradients and update the parameters in the network. Gradient calculation is complicated which is easy to incur mistakes.\n","tf.GradientTape is usually used to record forward calculation in Tensorflow, and reverse this \"tape\" to obtain the gradient.\n","\n","A symbolic differentiation (math expression manipulation and generation) program finds the derivative of a given formula with respect to a specified variable, producing a new formula as its output. In general, symbolic mathematics programs manipulate formulas to produce new formulas, rather than performing numeric calculations based on formulas."]},{"cell_type":"code","metadata":{"id":"-nvaGj6v5VA-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600798218962,"user_tz":300,"elapsed":841,"user":{"displayName":"Henry Ruiz","photoUrl":"","userId":"11545310100327958034"}},"outputId":"1f849abd-ddf2-4cde-eb7f-3beae3ac1507"},"source":["import tensorflow as tf\n","import numpy as np \n","# https://scipy-lectures.org/packages/sympy.html#differentiation\n","# Calculate the derivative of f(x) = a*x**2 + b*x + c\n","x = tf.Variable(55.0,name = \"x\",dtype = tf.float32)\n","a = tf.constant(1.0)\n","b = tf.constant(-2.0)\n","c = tf.constant(1.0)\n","\n","with tf.GradientTape() as tape:\n","    y = a*tf.pow(x,2) + b*x + c\n","    \n","dy_dx = tape.gradient(y,x)\n","print(dy_dx)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(108.0, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6QxpXlSJA5GX","colab_type":"text"},"source":["# calculate multiple derivatives\n"]},{"cell_type":"code","metadata":{"id":"DP_8K4wGANWA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1600798284090,"user_tz":300,"elapsed":431,"user":{"displayName":"Henry Ruiz","photoUrl":"","userId":"11545310100327958034"}},"outputId":"3c008876-7d7a-4fc9-9592-c6cc4a58a5d2"},"source":["with tf.GradientTape() as tape:\n","    tape.watch([a,b,c])\n","    y = a*tf.pow(x,2) + b*x + c    \n","dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])\n","print(dy_da)\n","print(dy_dc)\n","print(dy_da)\n","print(dy_dx)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(3025.0, shape=(), dtype=float32)\n","tf.Tensor(1.0, shape=(), dtype=float32)\n","tf.Tensor(3025.0, shape=(), dtype=float32)\n","tf.Tensor(108.0, shape=(), dtype=float32)\n"],"name":"stdout"}]}]}